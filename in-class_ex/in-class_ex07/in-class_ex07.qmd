---
title: "In-Class Exercise 7: Global and Local Measures of Spatial Association + Emerging Hot Spot Analysis (sfdep methods)"
date: "20 February 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

# Setup

## Installing and Loading R Packages

```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse)
```

## The Data

| Type       | Name                                   | Format | Description                                            |
|-----------------|-----------------|-----------------|---------------------|
| Geospatial | Hunan Province Administrative Boundary | .shp   | Data is at the County level.                           |
| Aspatial   | Hunan_2012                             | .xlsx  | Selected Hunan's local development indicators in 2012. |

## Importing Geospatial Data

```{r}
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

`st_read()` of **sf** package is used to import shapefile into R as a simple features object. Please refer to the documentation [here](https://r-spatial.github.io/sf/reference/st_read.html).

## Importing Aspatial Data

```{r}

hunan2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

# Data Wrangling

## Performing relational join

```{r}
hunan <- left_join(hunan,hunan2012) %>%
  select(1:4, 7, 15)

# auto takes the columns that exist in both objects
```

::: callout-note
The above code chunk will be used to update the attribute table of `hunan`'s SpatialPolygonsDataFrame with the attribute field of `hunan2012` dataframe. This is performed by using `left_join()` of **dplyr** package.
:::

# Part 1: Global and Local Measures of Spatial Association

## Basic ESDA

### Visualising Regional Development Indicator

We will prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using `qtm()` of **tmap** package.

```{r}
tmap_mode("plot")

tm_shape(hunan)+
  tm_fill("GDPPC", 
          style = "quantile", 
          palette = "Blues",
          title = "GDPPC") +
  tm_layout(main.title = "Distribution of GDP per capita by district, Hunan Province",
            main.title.position = "center",
            main.title.size = 0.8,
            legend.height = 0.35, 
            legend.width = 0.25,
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 1) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)

# remember tm_fill and tm_borders will give you the tm_polygon, we do it like this to have a higher level of control on the visuals

# always output the map first to see where you can place the map components like scale bar, compass, etc.

# Classification Method: if you are designing for a regional economic study then you might want to use "equal interval" classification method. It depends on the purpose of our study.
```

## Global Spatial Autocorrelation

In this section, we will compute the global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.

### Computing Contiguity Spatial Weights

Before we can compute the global spatial autocorrelation statistics, we need to construct the spatial weights of the study area (to define the neighbourhood relationships between the counties in the study area).

#### Deriving contiguity weights: Queen's method

We will compute the contiguity weights by using `st_weights()` of **sfdep** package.

In the code chunk below, queen method is used to derive the contiguity weights.

```{r}
cw_queen <- hunan %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_weights(nb,
                         style="W"),
         .before = 1)
```

Note that `st_weights()` provides 3 arguments:

-   *nb*: A neighbour list object created by `st_contiguity()`

-   *style*: Default "W" for row standardized weights (sum over all links to n). Other options include "B", "C", "U", "minmax", "S".

    -   *B* is the basic binary coding,

    -   *C* is globally standardised (sums over all links to n),

    -   *U* is equal to *C* / number of neighbours (sum over all links to unity),

    -   while *S* is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).

-   *allow_zero*: if `TRUE`, assigns zero as lagged value to zone without neighbours.

::: callout-note
Take note that *nb* and *weight matrix* are stored as a list. If you want to use them (we'll do this later) you need to use `unlist()`
:::

### Computing Global Moran's I

```{r}
moranI <- global_moran(cw_queen$GDPPC,
                       cw_queen$nb,
                       cw_queen$wt)
```

::: callout-note
`moranI` is a tibble dataframe with two values. Normally, we don't compute the Global Moran's I we just perform the Global Moran's I test (since it includes test result and test statistic). See below code chunk.
:::

### Performing Global Moran's I test

```{r}
global_moran_test(cw_queen$GDPPC,
                  cw_queen$nb,
                  cw_queen$wt)
```

::: callout-note
The global Moran's I test gives us the p-value allows you to know if you have enough statistical evidence to reject the null hypothesis or not. In this case, we have 0.000001095 which is \< alpha value of 0.05. Hence, we have sufficient statistical evidence to reject the null hypothesis that the observed GDPPC is spatially independent.

In fact, the Moran's I statistic is positive which shows us that there is positive autocorrelation (i..e., clustering)...
:::

### Performing Global Moran's I permutation test

```{r}
# this is done to ensure the code is reproducible - if involves simulations, please do this.
set.seed(1234)

global_moran_perm(cw_queen$GDPPC,
                  cw_queen$nb,
                  cw_queen$wt,
                  nsim = 99)

# in this case, we are running 100 simulations
```

::: callout-note
We can see that the significance level changes (p-value is smaller)
:::

### Computing local Moran's I

```{r}
lisa <- cw_queen %>%
  mutate(local_moran = local_moran(
    GDPPC, nb, wt, nsim = 99),
    .before = 1) %>%
  unnest(local_moran)

lisa
```

::: callout-note
Here are what the different columns mean:

*ii* - Moran's I value

*e_ii* - Expected value of Moran's I

*var_ii* - Variance of Moran's I

*z_ii* - Standardisation of Moran's I

*p_ii* - Moran's I derived after simulation

*Btw, for Take-Home please use `mean` of lisa. The reasoning will be that it follows a somewhat Normal Distribution.*
:::

### Visualising local Moran's I

```{r}
tmap_mode("plot")

tm_shape(lisa) + 
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8))
```

::: callout-note
-   +ve values = positive autocorrelation

-   -ve values = negative autocorrelation
:::

### Visualising p-value of Moran's I

```{r}
tmap_mode("plot")

tm_shape(lisa) + 
  tm_fill("p_ii_sim") +
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8))
```

::: callout-note
Use *p_ii_sim* to be more accurate. You always want to use the one that has several trials.
:::

### Visualising local Moran's I

```{r}

lisa_sig <- lisa %>%
  filter(p_ii_sim < 0.05)

tmap_mode("plot")

tm_shape(lisa) +
  tm_polygons() + 
  tm_borders(alpha = 0.5) +
tm_shape(lisa_sig) + 
  tm_fill("mean") +
  tm_borders(alpha = 0.4)
```

::: callout-note
Prof. Kam has mentioned that this way to do it is quite messy\... We should use the hands-on method where we have both together (and not separate layers) and have a legend for insignificant counties as well.
:::

## Hot Spot and Cold Spot Area Analysis

### Computing local Moran's I

```{r}
HCSA <- cw_queen %>%
  mutate(local_Gi = local_gstar_perm(
    GDPPC, nb, wt, nsim = 99),
    .before = 1) %>%
  unnest(local_Gi)

HCSA
```

::: callout-note
In general we use G\* and not G. Furthermore, in this case, we use the `local_gstar_perm()` version.
:::

### Visualising Gi\*

```{r}
tmap_mode("plot")

tm_shape(HCSA) +
  tm_fill("gi_star") +
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8))
```

### Visualising p-value of HCSA

```{r}
tmap_mode("plot")

tm_shape(HCSA) +
  tm_fill("p_sim") +
  tm_borders(alpha = 0.5)
```

::: callout-note
Most of this are greater than 0.05. We should make some changes to only see the ones that have statistical significance.
:::

# Part 2: Emerging Hot Spot Analysis

```{r}
pacman::p_load(sf, sfdep, tmap, plotly, zoo)
```

```{r}
GDPPC <- read_csv("data/aspatial/Hunan_GDPPC.csv")
hunan <- st_read(dsn = "data/geospatial", 
                 layer = "Hunan")
```

```{r}

# to get structure that includes spatial entities

GDPPC_st <- spacetime(GDPPC, hunan,
                      .loc_col = "County",
                      .time_col = "Year")
```

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(
    nb = include_self(st_contiguity(geometry)),
    wt = st_weights(nb)
  ) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

### Computing Gi \*

```{r}
gi_stars <- GDPPC_nb %>%
  group_by(Year) %>%
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt, nsim = 99)) %>%
  tidyr::unnest(gi_star)
```

### Mann-Kendall Test

```{r}
cbg <- gi_stars %>%
  ungroup() %>%
  filter(County == "Changsha") |> # you can use any county
  select(County, Year, gi_star)
```

```{r}
ggplot()
```
