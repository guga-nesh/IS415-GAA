---
title: "Take Home Exercise 02: Spatio-temporal Analysis of COVID-19 Vaccination Trends at the Sub-district Level, DKI Jakarta"
date: "17 February 2023"
date-modified: "`r Sys.Date()`"
number-sections: true
format: html
execute: 
  echo: true
  eval: true
  warning: false
editor: visual
---

## Problem Context

In late December 2019, the COVID-19 outbreak was reported in Wuhan, China, which had subsequently affected 210 countries worldwide. COVID-19 can be deadly with a 2% case fatality rate.

In response to this, Indonesia commenced its COVID-19 vaccination program on 13 January 2021, and as of 5 February 2023, over 204 million people had received the first dose of the vaccine, and over 175 million people had been fully vaccinated, with Jakarta having the highest percentage of population fully vaccinated.

However despite its compactness, the cumulative vaccination rate are not evenly distributed within DKI Jakarta. The question is where are the sub-districts with a relatively higher number of vaccination rate and how have they changed over time.

The assignment can be found [here](https://is415-ay2022-23t2.netlify.app/th_ex2.html).

## Objectives

Exploratory Spatial Data Analysis (ESDA) holds tremendous potential to address complex problems facing society. In this study, we will need to use it to uncover the spatio-temporal trends of COVID-19 vaccination in DKI Jakarta namely:

-   Choropleth Mapping and Analysis

-   Local Gi\* Analysis

-   Emerging Hot Spot Analysis (EHSA)

## Setup

### Packages used

These are the R packages that we'll need for our analyses:

-   **sf** - used for importing, managing, and processing geospatial data

-   **sfdep** - an `sf` and `tidyverse` friendly interface to compute spatial dependence

-   **tidyverse** - collection of packages for performing data science tasks such as importing, wrangling, and visualising data

-   **tmap** - for plotting cartographic quality static point patterns maps or interactive maps by using [leaflet](https://leafletjs.com/) API

-   **readxl** - for reading .xlsx files. It is part of the `tidyverse` collection.

-   **hash** - used to create a hash object. I have used this to convert the month spelling from Bahasa Indonesia to English. This is done so that when the data is visualised it is clear to all.

-   **moments** - used to call the `skewness()` function to calculate skewness of the data.

```{r}
pacman::p_load(sf, sfdep, tidyverse, tmap, hash, moments, plyr, readxl)
```

### Datasets used

+------------+--------------------------------------------------------------------------------------------------------------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Type       | Name                                                                                                                           | Format | Description                                                                                                                                                                                   |
+============+================================================================================================================================+========+===============================================================================================================================================================================================+
| Geospatial | [DKI Jakarta Provincial Village Boundary](https://www.indonesia-geospasial.com/2020/04/download-shapefile-shp-batas-desa.html) | .shp   | District level boundary in DKI Jakarta. Please take note that the data is from 2019.                                                                                                          |
+------------+--------------------------------------------------------------------------------------------------------------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Aspatial   | [District Based Vaccination History](https://riwayat-file-vaksinasi-dki-jakarta-jakartagis.hub.arcgis.com/)                    | .xlsx  | Daily .csv files containing all vaccinations done at the sub-district level (i.e., kelurahan) in DKI Jakarta. The files consider the number of doses given to the following groups of people: |
|            |                                                                                                                                |        |                                                                                                                                                                                               |
|            |                                                                                                                                |        | -   Mutual Cooperation Vaccination (Ages 18+)                                                                                                                                                 |
|            |                                                                                                                                |        |                                                                                                                                                                                               |
|            |                                                                                                                                |        | -   Vaccination of Health Workers (Ages 18+)                                                                                                                                                  |
|            |                                                                                                                                |        |                                                                                                                                                                                               |
|            |                                                                                                                                |        | -   Elderly Vaccination (Ages 18+)                                                                                                                                                            |
|            |                                                                                                                                |        |                                                                                                                                                                                               |
|            |                                                                                                                                |        | -   Public Service Vaccination (Ages 18+)                                                                                                                                                     |
|            |                                                                                                                                |        |                                                                                                                                                                                               |
|            |                                                                                                                                |        | -   Stage 3 Vaccination (General Community, Pre-Elderly, Vulnerable)                                                                                                                          |
|            |                                                                                                                                |        |                                                                                                                                                                                               |
|            |                                                                                                                                |        | -   Youth Vaccination (Ages 12-17)                                                                                                                                                            |
+------------+--------------------------------------------------------------------------------------------------------------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

## Data Wrangling: Geospatial Data

### Importing Geospatial Data

```{r}
jakarta <- st_read(dsn="data/geospatial",
                   layer="BATAS_DESA_DESEMBER_2019_DUKCAPIL_DKI_JAKARTA")
```

Information gathered on `jakarta` :

-   Data Type: **sf** collection

-   Geometry Type: Multipolygon

-   Shape: 269 features, 161 fields

-   CRS: WGS 84 - 'The World Geodetic System (WGS)'.

::: callout-note
The assigned CRS does not seem appropriate. In fact, the assignment requires us to re-assign the CRS to DGN95. We'll fix this in Section 4.2.3.
:::

### Data pre-processing

Before we can visualise our data, we need to ensure that the data is validated by handling invalid geometries and missing values.

*This section was done by referencing sample submissions done by two seniors, credit to:*

-   [*Xiao Rong Wong's 'Detecting Spatio-Temporal Patterns of COVID-19 in Central Mexico'*](https://rpubs.com/xiaorongw/IS415_Take-home_Ex02)

-   [*Megan Sim Tze Yen's 'Analysing and Visualising Spatio-temporal Patterns of COVID-19 in DKI Jakarta, Indonesia'*](https://is415-msty.netlify.app/posts/2021-09-10-take-home-exercise-1/#data-pre-processing)

Afterwards, we can work on doing the pre-processing on the `jakarta` data as per the requirements of the assignment.

#### Handling invalid geometries

```{r}
length(which(st_is_valid(jakarta) == FALSE))
```

::: callout-note
`st_is_valid()` checks whether a geometry is valid. It returns a logical vector indicating for each geometries of `jakarta` whether it is valid. The documentation for this can be found [here](https://r-spatial.github.io/sf/reference/valid.html).
:::

From the above output, we can see that the geometry is valid ðŸ‘

#### Handling missing values

```{r}
jakarta[rowSums(is.na(jakarta))!=0,]
```

::: callout-note
`rowSums()` method calculates the sum of each row of a data frame. Please refer to the documentation [here](https://www.rdocumentation.org/packages/raster/versions/3.6-14/topics/rowSums).

`is.na()` is a generic method that indicates which elements are missing. Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/NA).

`jakarta` 'wrapper' prints rows that contain NA values.
:::

From the output shown above we can see that there are NA values ðŸ‘Ž

In fact, there are two rows with missing values. However, it's hard to tell which columns the NA values are in since the output is so long and messy. Let's just retrieve the columns with missing values.

```{r}
names(which(colSums(is.na(jakarta))>0))
```

::: callout-note
`colSum()` helps form sum for data frame by column. Please refer to the documentation [here](https://www.rdocumentation.org/packages/Matrix/versions/1.5-3/topics/colSums).

`which()` gives the TRUE indices of a logical object, allowing for array indices. Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/which).

`names()` is used to get or set the names of an object. Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/names).
:::

There are two rows with missing values in the columns shown above. Let's see what they mean... Google Translate tells us that: ðŸ¤”

-   "KAB_KOTA" = REGENCY_CITY

-   "KECAMATAN" = SUB-DISTRICT

-   "DESA_KELUR" = VILLAGE_KELUR

We can remove all rows with missing values under the "DESA_KELUR" (i.e., VILLAGE level) column since we are only interested in the sub-district and potentially city level data.

::: callout-note
Later we find out that the definition of these columns by Google Translate are not 100% accurate. But it's okay as this does not affect our analysis for now.
:::

*Nonetheless, it should be noted that in this particular case removing the missing values from any of the columns will yield the same result.*

```{r}
jakarta <- na.omit(jakarta, c("DESA_KELUR"))
```

::: callout-note
`na.omit()` is used to handle missing values in objects. In this case, we input the `object`(must be an R object) and the `target.colnames` (a vector of names for the target columns to operate upon, if present in `object`. Please refer to the documentation [here](https://www.rdocumentation.org/packages/photobiology/versions/0.10.6/topics/na.omit).
:::

Let's check if we have removed the rows with missing values:

```{r}
jakarta[rowSums(is.na(jakarta))!=0,]
```

Looks good! ðŸ˜Ž But we still have 3 other pre-processing steps to do...

#### Re-assign Coordinate Reference System (CRS)

The CRS of `jakarta` is WGS 84. The assignment requires us to use the national CRS of Indonesia [*( DGN95 / Indonesia TM-3 zone 54.1)*](https://epsg.io/23845) since our dataset is specific to Indonesia.

```{r}
st_crs(jakarta)
```

Let's re-assign using EPSG code 23845 and check the results.

```{r}
jakarta <- st_transform(jakarta, 23845)
st_crs(jakarta)
```

::: callout-note
`st_transform()` is used to transform or convert coordinates of a simple feature. Please refer to the documentation [here](https://search.r-project.org/CRAN/refmans/sf/html/st_transform.html).

`st_crs()` is used to retrieve the coordinate reference system from an **sf** or **sfc** object. Please refer to the documentation [here](https://www.rdocumentation.org/packages/sf/versions/0.2-4/topics/st_crs).
:::

#### Exclude all outer islands from `jakarta`

The assignment requires us to remove all outer islands from `jakarta`. Let's plot the geometry of `jakarta` to see what we're working with.

```{r}
plot(jakarta['geometry'])
```

From the visualisation above, we can see that there are some outer islands scattered towards the north. Let's remove them.

In Section 4.2.2, we saw that the data is grouped by ***KAB_KOTA*** (i.e., CITY), ***KECAMATAN*** (i.e., SUB-DISTRICT), and ***DESA_KELUR*** (i.e., "VILLAGE"). Let's see the different cities we have in DKI Jakarta.

```{r}
unique(jakarta$KAB_KOTA)
```

The output shows us that there are 6 different cities in DKI Jakarta. 5 of them are prefixed with the string "JAKARTA" while one of them isn't. Let's plot this data to see if the one without the prefix represents the outer islands.

```{r}
tm_shape(jakarta) + 
  tm_polygons("KAB_KOTA")
```

It seems our suspicions are correct, the outer islands are those cities without the "JAKARTA" prefix. In fact, according to Google Translate, "KEPULAUAN SEBIRU" can be directly translated to "Thousand Islands" in English. Let's remove them and check the results.

```{r}
jakarta <- jakarta %>% filter(grepl("JAKARTA", KAB_KOTA))

unique(jakarta$KAB_KOTA)
```

::: callout-note
`grepl()` is used to search for matches to argument `pattern` (in our case its "JAKARTA") within each element of a character vector. Please find the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/grep).
:::

```{r}
plot(jakarta$geometry)
```

Looks good! ðŸ˜Ž

#### Retain first nine fields in `jakarta`

The assignment requires us to only retain the first 9 fields of the **sf** data frame. Let's do this.

```{r}
jakarta <- jakarta[,0:9]

jakarta
```

From the assignment we know that the ninth field is "JUMLAH_PEN". The output generated above matches this information.

#### Rename Columns for better understanding

For better understanding of the data, let us rename the columns to their English translation. For this we will use the `rename()` function.

::: callout-note
Since Google Translate has been giving me contradicting answers ðŸ˜“, I have decided to use this [wiki page](https://en.wikipedia.org/wiki/Subdivisions_of_Indonesia) as a guide.
:::

```{r}
# take note of the hierarchy of subdivisions of Indonesia

jakarta <- jakarta %>%
  dplyr::rename(
    object_id = OBJECT_ID,
    village_code = KODE_DESA,
    
    # fifth level
    village = DESA,
    code = KODE,
    
    # first level
    province = PROVINSI,
    
    # second level
    city = KAB_KOTA,
    
    # third level
    district = KECAMATAN,
    
    # fourth level - assumption made: KELUR = KELURAHAN
    sub_district = DESA_KELUR,
    total_population = JUMLAH_PEN
  )
```

::: callout-note
`rename()` is a method under the **dplyr** library. It changes the names of individual variables using the `new_name = old_name` syntax.

side note: `rename_with()` renames columns using a function. Please refer to the documentation [here](https://dplyr.tidyverse.org/reference/rename.html).
:::

We have completed the data pre-processing steps for the geospatial data ðŸ¥³ Let's move on to the aspatial data.

## Data Wrangling: Aspatial Data

In this section, we'll be importing and performing some basic pre-processing on the COVID-19 vaccination datasets.

### Importing Aspatial Data

The assignment requires us to compute the vaccination rate from July 2021 to June 2022 at the sub-district (i.e., kelurahan) level.

::: callout-important
We will be taking the data compiled on the [**last day**]{.underline} of each month to retrieve the monthly records of COVID-19 vaccinations from the above-mentioned time period.
:::

#### Primary Data Exploration

Since we will need to import 12 files (which is a lot), let's look at the structure of one of them to figure what kind of data we have and how we can pre-process it moving forward.

```{r}
july_2021 <- readxl::read_xlsx("data/aspatial/Data Vaksinasi Berbasis Kelurahan (31 Juli 2021).xlsx")

glimpse(july_2021)
```

::: callout-note
`read_xlsx()` reads data from a .xlsx file. This function is from the **readxl** library under the **tidyverse** collection.

`glimpse()` makes it easier to see every column in a data frame. Please refer to the documentation [here](https://dplyr.tidyverse.org/reference/glimpse.html).
:::

Information gathered on `july_2021`:

-   Data Type: **tibble** data frame

-   Shape: 268 entries, 27 columns

There seems to be a quite a few columns...ðŸ¤”

We will only need to take the "KELURAHAN" (i.e., sub- district level) , "SASARAN" (i.e., target) and the "BELUM VAKSIN" (i.e., number of people who have not been vaccinated yet) columns to calculate the vaccination rate for each month. We will look at how we can calculate this in Section 5.1.2.

*I learnt that only these columns were necessary for our analysis after receiving some advice from Professor Kam Tin Seong. Thank you so much for your guidance prof!* ðŸ˜ƒ

#### Creating a function to process our Aspatial Data

First, let us get all the file names for our aspatial data.

```{r}
setwd("data/aspatial")
file.list <- list.files(pattern = "*.xlsx")
file.list
```

::: callout-note
*The above code chunk was referenced from [stackoverflow](https://stackoverflow.com/questions/32888757/how-can-i-read-multiple-excel-files-into-r). Credit to [Jaap](https://stackoverflow.com/users/2204410/jaap) for his detailed answer there.*

`setwd("dir")` is used to set the working directory to string "dir". Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/getwd).

`library()` loads and attaches add-on packages. In this case, we need to use to `read_excel()` from **readxl**. Hence, we have loaded in that package. Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/library).

`list.files()` is used to generate a vector of file names in the specified directory (from `setwd()`) with the pattern ("\*.xlsx"). Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/list.files).
:::

Now, we are ready to create a function that processes all the files and merges all the data into a single sf data frame. Our output after all the pre-processing should look something like this:

![](images/image-196486236.png)

We will be using two functions to achieve the above-shown skeleton of the data frame:

-   `get_month_year()` - this is used to get the English month and year of each file. The output will be put into the final data frame under the "MONTH.YEAR" column

-   `aspatial_data_processing()` - this is used to process the data. The following steps are done:

    1.  Keep the necessary columns only (i.e., "KODE.KELURAHAN", "KELURAHAN", "SASARAN", "BELUM.VAKSIN"). We need the "KODE.KELURAHAN" column to join the processed aspatial data with the processed geospatial data. We will look at how to do this in Section 6.1.

    2.  Remove the first row of the **tibble data frame** as it is not needed (it contains the aggregate sum of the data)

    3.  Check for duplicates and remove them *(while primary exploration of data did not show duplicates, we must still be careful as not all files have been manually checked and there is a still a chance that duplicates exist)*

    4.  Check for invalid data (i.e., missing values/NA) and convert them to 0 for us to perform our calculations later on.

    5.  Calculate "VACCINATION.RATE"

    6.  Create "DATE" column (i.e., date of data collected)

::: callout-important
Formula for Vaccination Rate is as follows:

$vaccination rate = (target - unvaccinated) / target$
:::

```{r}
get_date <- function(filepath) {
  # create hash object to map Bahasa Indonesian translation to English
  h <- hash()
  h[["Januari"]] <- "01"
  h[["Februari"]] <- "02"
  h[["Maret"]] <- "03"
  h[["April"]] <- "04"
  h[["Mei"]] <- "05"
  h[["Juni"]] <- "06"
  h[["Juli"]] <- "07"
  h[["Agustus"]] <- "08"
  h[["September"]] <- "09"
  h[["Oktober"]] <- "10"
  h[["November"]] <- "11"
  h[["Desember"]] <- "12"
  
  # get components of filepath to get date
  components = str_split(filepath, " ")
  month = h[[components[[1]][6]]]
  year = gsub("[^0-9.-].", "", components[[1]][7])
  day = gsub("[^0-9.-]", "", components[[1]][5])
  date = as.Date(paste(year, month, day, sep = "-"))
  
  return (date)
}
```

::: callout-note
The **hash** library and method is used to create and work with has objects. Please refer to the documentation [here](https://www.rdocumentation.org/packages/hash/versions/3.0.1/topics/hash).

`str_split()` takes a character vector and returns a list. In this case, I used it to split up the `filepath` of type *\<string\>* so that I could extract the month and year. Please refer to the documentation [here](https://stringr.tidyverse.org/reference/str_split.html).

`gsub()` performs replacement of the values specified in regex with the value you have specified. In this case, I simply replaced all the non-numerical values with an empty string. Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/grep).
:::

```{r}
aspatial_data_processing <- function(filepath) {
  
  vaccine_data = read_xlsx(filepath, .name_repair = "universal")
  
  # only keep the necessary columns
  vaccine_data = vaccine_data[, c("KODE.KELURAHAN", "KELURAHAN", "SASARAN", "BELUM.VAKSIN")]

  # remove the first row which has the aggregate sum of vaccination numbers
  vaccine_data = vaccine_data[-1, ]

  # check for duplicates and remove them
  vaccine_data <- vaccine_data%>%
  distinct(KELURAHAN, .keep_all = TRUE)
  
  # check for invalid data (missing values or NA) and convert them to 0
  vaccine_data[is.na(vaccine_data)] <- 0

  
  # calculate vaccination rate - (sasaran - belum vaksin)/sasaran
  vaccine_data$VACCINATION.RATE = (vaccine_data$SASARAN - vaccine_data$BELUM.VAKSIN)/vaccine_data$SASARAN
  
  # create date column
  date <- get_date(filepath)
  vaccine_data$DATE <- date
  
  return(vaccine_data)
}
```

::: callout-note
The argument `.name_repair` of the `read_xlsx()` method is used to automatically change column names to a certain format. "universal" names are unique and syntactic. Please refer to the documentation [here](https://readxl.tidyverse.org/reference/read_excel.html#arguments).

To remove duplicate values of a certain column, we can use the `distinct()` method. This only works for **dplyr** \>= 0.5. Please refer to this [stackoverflow link](https://stackoverflow.com/questions/22959635/remove-duplicated-rows-using-dplyr) that explains more in-depth.

I have used the `is.na()` method to convert all NA values to 0. The `vaccine_data` wrapper is used to make sure we transform the data inside that variable. Please refer to the documentation [here](https://www.rdocumentation.org/packages/ursa/versions/3.9.4/topics/is.na).
:::

#### Running `aspatial_data_processing()` on Aspatial Data

```{r}
setwd("data/aspatial")

dflist <- lapply(seq_along(file.list), function(x) aspatial_data_processing(file.list[x]))
```

::: callout-note
`seq_along()` is a built-in function in R that creates a vector that contains a sequence of numbers from 1 to the object's length. You can find an in-depth explanation of this method [here](https://r-lang.com/seq_along-function-in-r/).

We used `seq_along()` to create a vector so we can use `lapply()` (which only works on lists or vectors) to apply the `aspatial_data_processing()` to all the files.
:::

```{r}
processed_aspatial_data <- ldply(dflist, data.frame)

glimpse(processed_aspatial_data)
```

::: callout-note
Here, we use the `ldply()` function from the **plyr** library to combine and return the results in a single data frame. Please refer to the documentation [here](https://www.rdocumentation.org/packages/plyr/versions/1.8.8/topics/ldply).
:::

#### Rename Columns for better understanding

```{r}
processed_aspatial_data <- processed_aspatial_data %>%
  dplyr::rename(
    village_code = KODE.KELURAHAN,
    sub_district = KELURAHAN,
    target = SASARAN,
    not_vaccinated = BELUM.VAKSIN,
    vaccination_rate = VACCINATION.RATE,
    date = DATE
  )
```

With that we have completed the Data Wrangling processes for the Aspatial Data ðŸ˜Ž

Now, we can finally start analysing the data proper ðŸ“ˆ

## Choropleth Mapping and Analysis (Task 1)

In Section 2 we looked at the three main objectives of this assignment. Let's start with the first one. *Please take note that we have already calculated the monthly vaccination rate from July 2021 to June 2022 in section 5.1.2.*

Before we can create the choropleth maps to analyse the data we will need to combine the Spatial and Aspatial data.

### Combining both data frames using `left_join()`

We would like to join the two data sets on the "village_code" column. Let's take a look at the data to see if there are any discrepancies.

```{r}
setdiff(processed_aspatial_data$village_code, jakarta$village_code)
```

::: callout-note
`setdiff()` finds the (asymmetric) difference between two collections of objects. Please refer to the documentation [here](https://www.rdocumentation.org/packages/nat/versions/1.6.6/topics/setdiff).
:::

Hm... ðŸ¤” while this might seem confusing at first we need to remember that we removed the outer islands from the Geospatial data in Section 4.2.4 but we didn't do that for the Aspatial data. We don't have to do this since `left_join()` helps us exclude these data automatically.

In fact, here are what these codes mean:

-   "3101011003" - PULAU HARAPAN

-   "3101011002" - PULAU KELAPA

-   "3101011001" - PULAU PANGGANG

-   "3101021003" - PULAU PARI

-   "3101021002" - PULAU TIDUNG

-   "3101021001" - PULAU UNTUNG JAWA

```{r}
jakarta_vaccination_rate <- left_join(jakarta, processed_aspatial_data, by = "village_code")
```

::: callout-note
`left_join()` is a function from the **dplyr** library that returns all rows from X (i.e., `jakarta`) and all columns from X and Y (i.e., `processed_aspatial_data`). Please refer to the documentation [here](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/join).
:::

### Mapping the Distribution of Vaccination Rate for DKI Jakarta over 12 months

#### Choosing the Data Classification Method

There are many data classification methods but the Natural Breaks (or "Jenks") method is useful as the class breaks are iteratively created in a way that best groups similar values together and maximises the differences between classes. However this classification is not recommended for data with low variances ([source](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization#:~:text=The%20Jenks%20classification%20method%20is,values%2C%20it%20will%20identify%20them.)).

We can use the Coefficient of Variation (CV) to check whether or variance is large or small. As a rule of thumb a CV \>= 1 indicates a relatively high variation, while CV \< 1 can be considered low ([source](https://statisticsbyjim.com/basics/coefficient-variation/)).

```{r}
# create a helper function to calculate CV for the different month_year(s)
print_variance <- function(data) {
  different_time_periods = unique(data$date)
  for (time_period in different_time_periods) {
    temp = filter(data, `date` == time_period)
    print(paste(time_period, '-', sd(temp$vaccination_rate)/mean(temp$vaccination_rate)))
  }
}
```

::: callout-note
`paste()` is used to concatenate vectors by converting them into characters. Here we have used it to print more than 1 variable. Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/paste).

`sd()` computes the standard deviation of the values in `temp$vaccination_rate`. Please refer to the documentation [here](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/sd).

`mean()` computes the mean of the values in `temp$vaccination_rate`. Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/mean).
:::

```{r}
# call function created in the previous code chunk
print_variance(jakarta_vaccination_rate)
```

From the above output, we can see that the CV for the vaccination_rate is pretty low. So maybe "Jenks" isn't the best method.

Another data classification method is the "Equal Interval" method. As Professor Kam has mentioned in his [slides](https://is415-ay2022-23t2.netlify.app/lesson/lesson03/lesson03-thematic_and_analytical_mapping#/9), this method divides the `vaccination_rate` values into equally sized classes. However, we should avoid this method if the data is skewed to one end or if we have 1 or 2 really large outlier values.

So let's check our data if it is skewed and if it has any outliers. For this, we will be using the `skewness()` and `hist()` methods to calculate the skewness and visualise the data respectively. Do note that as a rule of thumb if the skewness value is between -0.5 and 0.5 the data is relatively symmetrical.

```{r}
# create a helper function to calculate skewness for the different month_year(s)
print_skewness <- function(data) {
  different_time_periods = unique(data$date)
  for (time_period in different_time_periods) {
    temp = filter(data, `date` == time_period)
    print(paste(time_period, '-', moments::skewness(temp$vaccination_rate)))
  }
}
```

::: callout-note
`skewness()` computes the skewness of the data given (i.e., `temp$vaccination_rate`). Please refer to the documentation [here](https://www.rdocumentation.org/packages/e1071/versions/1.7-13/topics/skewness).
:::

```{r}
# call function created in the previous code chunk
print_skewness(jakarta_vaccination_rate)
```

From the above output we can see that all the data is relatively symmetrical. ðŸ˜Œ

Let's plot the histograms to check if outliers are present as well!

```{r}
# create a helper function to draw histogram for the different month_year(s)
hist_plot <- function(df, time_period) {
hist(filter(jakarta_vaccination_rate, `date` == time_period)$vaccination_rate, ylab=NULL, main=time_period, xlab=NULL)
}
```

::: callout-note
`hist()` computes a histogram of the given values. Please refer to the documentation [here](https://www.rdocumentation.org/packages/graphics/versions/3.6.2/topics/hist).

We used this method as it is the easiest way to detect outliers ([source](https://www.reneshbedre.com/blog/find-outliers.html#histogram-scatterplot-and-boxplot)). Especially if the dataset is quite small.
:::

```{r}
#| code-fold: true
# call function created in the previous code chunk
par(mfrow = c(3,2))
hist_plot(jakarta_vaccination_rate, as.Date("2021-07-31"))
hist_plot(jakarta_vaccination_rate, as.Date("2021-08-31"))
hist_plot(jakarta_vaccination_rate, as.Date("2021-09-30"))
hist_plot(jakarta_vaccination_rate, as.Date("2021-10-31"))
hist_plot(jakarta_vaccination_rate, as.Date("2021-11-30"))
hist_plot(jakarta_vaccination_rate, as.Date("2021-12-31"))
```

```{r}
#| code-fold: true
# call function created in the previous code chunk
par(mfrow = c(3,2))
hist_plot(jakarta_vaccination_rate, as.Date("2022-01-31"))
hist_plot(jakarta_vaccination_rate, as.Date("2022-02-27"))
hist_plot(jakarta_vaccination_rate, as.Date("2022-03-31"))
hist_plot(jakarta_vaccination_rate, as.Date("2022-04-30"))
hist_plot(jakarta_vaccination_rate, as.Date("2022-05-31"))
hist_plot(jakarta_vaccination_rate, as.Date("2022-06-30"))
```

From our findings, we can say that the data is not skewed and there are no major outliers that we need to worry about ðŸ¥³ Hence, the "Equal Interval" method will be perfect for our choropleth maps.

#### Spatio-Temporal Mapping with custom breakpoints

To analyse the spatio-temporal change in vaccination rates over time we should use the same data classification method and use the same class intervals. This makes it easier for any comparisons to be made.

-   The Equal Interval data classification method will be utilised (as discussed in Section 6.2.1).

-   We will need to define our custom breaks to have a common classification scheme for the COVID-19 vaccination rates.

Let's take a look at the summary statistics for vaccination rates across the 12 months to determine the breakpoints:

```{r}
# summary stats for Vaccination Rates in July 2021
summary(filter(jakarta_vaccination_rate, `date` == as.Date("2021-07-31"))$vaccination_rate)
```

```{r}
# summary stats for Vaccination Rates in November 2021
summary(filter(jakarta_vaccination_rate, `date` == as.Date("2021-11-30"))$vaccination_rate)
```

```{r}
# summary stats for Vaccination Rates in March 2022
summary(filter(jakarta_vaccination_rate, `date` == as.Date("2022-03-31"))$vaccination_rate)
```

```{r}
# summary stats for Vaccination Rates in June 2022
summary(filter(jakarta_vaccination_rate, `date` == as.Date("2022-06-30"))$vaccination_rate)
```

::: callout-note
`summary()` is a generic function used to produce result summaries as shown above. Please refer to the documentation [here](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/summary). We used this here to find out the range of vaccination rates over the 12 months. We can see that the range is from 0.3701 to 0.8978.
:::

Hence, breakpoints can be defined with intervals of 0.1 where values range from 0.3 to 0.9.

Since we need to plot a map for each month let's start by creating a generic helper function to do it.

```{r}
# create a helper function to plot the choropleth map
choropleth_plotting <- function(df, d) {
  temp = filter(df, `date` == d)
  tm_shape(temp)+
  tm_fill("vaccination_rate",
          breaks = c(0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9),
          palette = "Blues",
          title = "Vaccination Rate",
          legend.hist = TRUE) +
  tm_layout(main.title = format(d, "%b-%Y"),
            main.title.position = "center",
            main.title.size = 0.8,
            legend.height = 0.40, 
            legend.width = 0.25,
            frame = TRUE) +
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 1) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
}
```

```{r}
#| code-fold: true
# call the functions for all the different months
tmap_mode("plot")
tmap_arrange(choropleth_plotting(jakarta_vaccination_rate, as.Date("2021-07-31")),
             choropleth_plotting(jakarta_vaccination_rate, as.Date("2021-08-31")))
```

```{r}
#| code-fold: true
# call the functions for all the different months
tmap_mode("plot")
tmap_arrange(choropleth_plotting(jakarta_vaccination_rate, as.Date("2021-09-30")),
             choropleth_plotting(jakarta_vaccination_rate, as.Date("2021-10-31")))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(choropleth_plotting(jakarta_vaccination_rate, as.Date("2021-11-30")),
           choropleth_plotting(jakarta_vaccination_rate, as.Date("2021-12-31")))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(choropleth_plotting(jakarta_vaccination_rate, as.Date("2022-01-31")),
             choropleth_plotting(jakarta_vaccination_rate, as.Date("2022-02-27")))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(choropleth_plotting(jakarta_vaccination_rate, as.Date("2022-03-31")),
             choropleth_plotting(jakarta_vaccination_rate, as.Date("2022-04-30")))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(choropleth_plotting(jakarta_vaccination_rate, as.Date("2022-05-31")),
             choropleth_plotting(jakarta_vaccination_rate, as.Date("2022-06-30")))
```

### Observations from Choropleth Maps with Custom Equal Interval Breakpoints

![*The above gif was made by stitching the individual choropleths we created earlier on [ezgif](https://ezgif.com/maker).*](images/ezgif.com-optimize.gif)

-   There was an increase in vaccination rate all over Jakarta. There doesn't seem to be a single point from which it starts. However, it should be noted that top 3 sub-districts with the highest vaccination rates in July 2021 were **Kamal Muara**, **Halim Perdana Kusuma**, and **Kelapa Gading Timur**. While Halim Perdana Kusuma was still in the top 3 after a year, Kamal Muara and Kelapa Gading Timur were replaced by **Srengseng Sawah** and **Mabggarai Selatan**. Halim Perdana Kusuma remained one of the highest as it had a relatively low target of 28363.

-   After a year, the lowest vaccination rate in a sub-district was 78%. With this, we can see that the generally the population of Jakarta was very receptive to the COVID-19 vaccines.

-   Additionally, the gif tells us that the receptiveness of the vaccine seem to spread "inwards" where the bigger sub-districts tend to reach a higher vaccination rate quicker and gaps between them (i.e., the smaller sub-districts) were "filled-in" later on. Therefore, unequal distribution of vaccination rates among the sub-districts could possibly be attributed to their size. The Indonesian government might have focused their efforts on the largely populated areas to attain herd immunity quicker.

With that, we have completed Task 1 of our Take-Home Exercise ðŸ¥³ Let's move onto the next task.

## Local Gi\* Analysis (Task 2)

While choropleth maps are a useful visualisation tool for displaying spatial data, they have limitations when it comes to identifying clusters or hotspots of high and low vaccination rates. To do so, we can employ statistical methods such as the **Local Getis-Ord G\*** statistic. It measures whether a particular region has values that are significantly higher or lower than its neighbouring regions.

First we need to create a time-series cube using `as_spacetime()`. The **spacetime** class links a flat data set containing spatio-temporal information with the related geometry data ([source](https://sfdep.josiahparry.com/articles/spacetime-s3.html)).

### Create function to calculate Local Gi\* values of the monthly vaccination rate

To calculate the Gi\* statistic values of the vaccination rates, we need to construct the spatial weights of the study area. This is done to define the neighbourhood relationships between the sub-districts in DKI Jakarta.

Since our study is focused on identifying clusters of high or low vaccination rates at a local scale, it is more appropriate to use an adjacency criterion. We will be defining our contiguity weights using the **Queen's (Kings) Case**. Additionally, the **Inverse Distance** spatial weighting method will be used as it takes into account the spatial autocorrelation of the vaccination rates ([source](https://is415-ay2022-23t2.netlify.app/lesson/lesson07/lesson07-glsa#/33)). This is to ensure the influence of neighbouring locations on a target location is accounted for.

Then we will need to compute the local Gi\* statistics using `local_gstar_perm()`. Our significance level will be set to 0.05.

```{r}

gi_values <- function (d) {
  set.seed(1234)
  
  temp = filter(jakarta_vaccination_rate, `date` == d)
  
  # compute conguity weights
  cw <- temp %>%
  mutate(nb = st_contiguity(geometry),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1))
  
  # compute Gi* values
  HCSA <- cw %>%
    mutate(local_Gi=local_gstar_perm(
      vaccination_rate, nb, wt, nsim=99),
      .before=1) %>%
    unnest(local_Gi)
  
  return (HCSA)
}

```

```{r}
# run the function for all the different dates

dates = c("2021-07-31", "2021-08-31", "2021-09-30", "2021-10-31", "2021-11-30", "2021-12-31", "2022-01-31", "2022-02-27", "2022-03-31", "2022-04-30", "2022-05-31", "2022-06-30")

gi_output = list()

for (i in 1: 12) {
  gi_output[[i]] = gi_values(dates[i])
}

# GI* values have been computed
```

::: callout-note
`st_contiguity()` is used to generate the neighbour list. Please refer to the documentation [here](https://rdrr.io/github/JosiahParry/sfweight/man/st_contiguity.html).

`st_inverse_distance()` helps calculate inverse distance weight from the neighbour list and geometry column. Please refer to the documentation [here](https://sfdep.josiahparry.com/reference/st_inverse_distance.html).

`local_gstar_perm()` helps us calculate the local Gi\* statistic value. Please refer to the documentation [here](https://sfdep.josiahparry.com/reference/local_gstar.html).

`unnest()` expands a list-column containing data frames into rows and columns. Please refer to the documentation [here](https://tidyr.tidyverse.org/reference/unnest.html).
:::

### Create a function to display Gi\* maps

As requested, when creating the maps, we will need to exclude the insignificant data (i.e., `p-sim` \< 0.05).

```{r}
# function to draw maps
gi_map <- function(gi_output) {
  tm_shape(gi_output) +
  tm_polygons() +
  tm_shape(gi_output %>% filter(p_sim < 0.05)) +
  tm_fill("gi_star") + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = paste("Gi values of", gi_output$date[1]),
            main.title.size = 1)
}
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(gi_map(gi_output[[1]]),
             gi_map(gi_output[[2]]))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(gi_map(gi_output[[3]]),
             gi_map(gi_output[[4]]))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(gi_map(gi_output[[5]]),
             gi_map(gi_output[[6]]))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(gi_map(gi_output[[7]]),
             gi_map(gi_output[[8]]))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(gi_map(gi_output[[9]]),
             gi_map(gi_output[[10]]))
```

```{r}
#| code-fold: true
tmap_mode("plot")
tmap_arrange(gi_map(gi_output[[11]]),
             gi_map(gi_output[[12]]))
```

### Observations on Gi\* Maps of monthly vaccination rate

As we have learnt in [class](https://is415-ay2022-23t2.netlify.app/lesson/lesson07/lesson07-glsa#/28), if the Gi\* value is significant and positive, the location is associated with relatively high values of the surrounding locations and the opposite is true. With this information, we can make these observations:

-   Generally throughout the year the hot spots (green colour) seem to be more clustered compared to the cold spots (red colour). In fact, the cold spots appear to be more well spread out (with some exceptions which I will talk about in the next point). In addition, most of the hot spot sub-districts are larger than the cold spots sub-districts. In Section 6.3, I theorised that the Indonesian government might have focused their efforts on largely populated sub-districts to attain herd immunity quicker. These maps further support that point as it shows us that the higher values tend to cluster around the larger districts.

-   As mentioned, the cold spots appear to be less clustered and well spread out. However, we should note that the center of DKI, Jakarta seems to be an exception. Throughout the 12 months, the center shows a few cold spots. In Section 6.2, our maps show us that that region has the lowest vaccination rates. Hence, we can infer that there is some underlying process that is influencing the variable such as lack of quality health care systems in that area. The Indonesian government should look into this.

With that, we have completed Task 2 of our Take-Home Exercise ðŸ¥³ Let's move onto the next task.

## Emerging Hot Spot Analysis (Task 3)

### Creating a Time Series Cube

Before we can perform our EHSA, we need to create a time series cube. The **spacetime** class links a flat data set containing spatio-temporal data with the related geometry. This allows us to examine changes in a variable over time while accounting for the spatial relationships between locations.

```{r}
temp = select(jakarta_vaccination_rate, c(2, 8, 13, 14)) %>% st_drop_geometry()
```

```{r}
vr_st <- spacetime(temp, jakarta,
                   .loc_col = "village_code",
                   .time_col = "date")
```

Let's check if `vr_st` is a spacetime cube.

```{r}
is_spacetime_cube(vr_st)
```

::: callout-note
`spacetime()` creates new **spacetime** objects. Please refer to the documentation [here](https://sfdep.josiahparry.com/articles/version-0-1-2?q=spacetime#spacetime).

`is_spacetime_cube()` determines if a **spacetime** object is a spatio-temporal full grid (i.e., spacetime cube). Please refer to the documentation [here](https://sfdep.josiahparry.com/reference/is_spacetime_cube.html).
:::

With the confirmation that `vr_st` is indeed a **spacetime cube** object, we can proceed with our analysis.

### Computing Gi\*

#### Deriving spatial weights

We will use the same method used in Section 7.1. We will be using the inverse distance method to identify neighbours and derive the spatial weights.

```{r}
vr_nb <- vr_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_wts("wt") %>%
  set_nbs("nb")
```

```{r}
head(vr_nb)
```

#### Calculating the Gi\* values

With the `nb` and `wt` columns created by the above code chunk we can calculate the Local Gi\* for each location like so:

```{r}
gi_stars <- vr_nb %>% 
  group_by(`date`) %>% 
  mutate(gi_star = local_gstar_perm(
    vaccination_rate, nb, wt)) %>% 
  tidyr::unnest(gi_star)
```

### Using Mann-Kendall Test to evaluate trends in each location

The Mann-Kendall statistical test is used to assess the trend of a set of data values and whether the trend (in any direction) is statistically signification ([source](https://is415-ay2022-23t2.netlify.app/lesson/lesson07/lesson07-glsa#/37)). The test calculates a Kendall **tau** rank correlation coefficient (measures strength and direction of trend) and a **p-value** (indicates statistical significance of the trend).

I have selected the following sub-districts as per the requirements of this assignment (I chose these sub-districts based off my analyses in Section 6.3:

-   KOJA - 3172031001

-   SRENGSENG SAWAH - 3174091002

-   MANGGARAI SELATAN - 3174011006

```{r}
k <- gi_stars %>%
  ungroup() %>%
  filter(village_code == 3172031001) %>%
  select(village_code, date, gi_star)
```

```{r}
ss <- gi_stars %>%
  ungroup() %>%
  filter(village_code == 3174091002) %>%
  select(village_code, date, gi_star)
```

```{r}
ms <- gi_stars %>%
  ungroup() %>%
  filter(village_code == 3174011006) %>%
  select(village_code, date, gi_star)
```

Mann-Kendall Test details:

-   **H0**: There is no monotonic trend in the series.

-   **H1**: There is a trend in the series.

-   **Confidence Level** = 95%

    -   I have chosen this level of confidence as it strikes a balance between the level of precision and the level of certainty that is desired in the results. In fact, generally a confidence level of 99% is not commonly used in practice, given the inherent uncertainty in many research studies.

-   **Significance Level** = 0.05

```{r}
ggplot(data = k,
       aes(x = date,
           y = gi_star)) +
  geom_line() +
  theme_light() +
  ggtitle("KOJA")
```

```{r}
k %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

```{r}
ggplot(data = ss, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light() + 
  ggtitle("SRENGSENG SAWAH")
```

```{r}
ss %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

```{r}
ggplot(data = ms, 
       aes(x = date, 
           y = gi_star)) +
  geom_line() +
  theme_light() + 
  ggtitle("MANGGARAI SELATAN")
```

```{r}
ms %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>% 
  tidyr::unnest_wider(mk)
```

### Observations on Mann-Kendall Test for the 3 sub-districts selected

-   **KOJA**: We see a sharp upwards trend in the first 4 months. However, after Oct 2021, we can see indications of a slowing rate of growth in the vaccination rate. Since the tau value is positive and very close to 1 we can see that there is a strong increasing trend. However, since the p-value is less than 0.05 which indicates that the trend is statistically significant and we can reject the null hypothesis that there is no trend.

-   **SRENGSENG SAWAH**: Similar to Koja, we see a sharp upwards trend within the first 4 months. However, after Oct 2021, we can see indications of a plateauing raise. We should note that the plauteauing of the rate of increase in Srengseng Sawah seems to be worse than in Koja. The tau value is positive and close to 1 which shows a strong upwards trend. Since the p-value is below 0.05 we cannot reject the null hypothesis that there is no monotonic trend in the series.

-   **MANGGARAI SELATAN**: Similar to Koja and Srengseng Sawah, we see a sharp upwards trend within the first 4 months. However, after Oct 2021, we can see indications that the growth in vaccination rate is tapering off. It should be notes that Manggarai Selatan has the most gradual decrease in vaccination rate increase when compares with Koja and Srengseng Sawah. We see a strong upwards trend since the tau value is positive and close to 1. Since the p-value is \< 0.05 the trend is statistically significant and we can reject the null hypothesis.

-   These chart further supports our analysis done in Section 6.3 that vaccination rate increase seem to taper off over time. This is likely due to the different sub-districts meeting their vaccination targets and the relaxation of COVID-19 measures since the start of 2022 ([source](https://www.reuters.com/world/asia-pacific/indonesia-lifts-remaining-covid-restrictions-president-2022-12-30/)).

#### Performing Emerging Hotspot Analysis

By using the below code, we can replicate the Mann-Kendall test on all the `village_code`(s) and even identify the significant ones.

*The code used in this section was provided by Professor Kam Tin Seong in our In-Class Exercise [here](https://is415-gaa-tskam.netlify.app/in-class_ex/in-class_ex07/in-class_ex07_ehsa#arrange-to-show-significant-emerging-hotcold-spots).*

```{r}
ehsa <- gi_stars %>%
  group_by(village_code) %>%
  summarise(mk = list(
    unclass(
      Kendall::MannKendall(gi_star)))) %>%
  tidyr::unnest_wider(mk)
```

```{r}
emerging <- ehsa %>% 
  arrange(sl, abs(tau))

emerging
```

### Prepare EHSA map of Gi\* values of vaccination rate

This will be done using `emerging_hotspot_analysis()`

```{r}
set.seed (1234)
```

```{r}
ehsa <- emerging_hotspot_analysis(
  x = vr_st, 
  .var = "vaccination_rate", 
  k = 1, 
  nsim = 99
)
```

```{r}
ggplot(data = ehsa,
       aes(x=classification)) +
  geom_bar()
```

```{r}
jakarta_ehsa <- jakarta %>%
  left_join(ehsa,
            by = join_by(village_code == location))
```

```{r}
ehsa_sig <- jakarta_ehsa  %>%
  filter(p_value < 0.05)
tmap_mode("plot")
tm_shape(jakarta_ehsa) +
  tm_polygons() +
  tm_borders(alpha = 0.5) +
tm_shape(ehsa_sig) +
  tm_fill("classification") + 
  tm_borders(alpha = 0.4)
```

### Observations of Emerging Hot Spot Analysis

There is a large number of oscilating hotspots which indicates that most of the areas in DKI Jakarta is experiencing intermittent increases in COVID-19 vaccination. Do note that these areas tend to be the large sub-districts which further proves our points made in the earlier sections that more people have been getting vaccinated quickly in these states. This could be due to effective vaccine campaigns by the government or the population in these areas tend to be more wary of the spread of COVID-19 and have decided to get vaccinated as the vaccines are made available.

There is also a large number of sporadic coldspots which suggest that there is a statistically significant decrease in the vaccination rates. This means that these areas have lower vaccine uptake than in their surrounding areas. While the number of these sub-districts are lower it should still be a cause of concern as there are lower vaccination rates in these areas. Identifying and addressing these areas is important for public health officials to ensure that vaccine uptake is equitable and that vulnerable populations are not left behind in their vaccination efforts.

It seems that there are more sporadic coldspots clustered in the center of Jakarta while the oscilating hotspots are surrounding the these areas.
